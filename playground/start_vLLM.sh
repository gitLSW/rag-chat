vllm serve \
    --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \
    --task auto \
    --tokenizer <model> \
    --hf-config-path <model> \
    --skip-tokenizer-init False \
    --revision <default> \
    --code-revision <default> \
    --tokenizer-revision <default> \
    --tokenizer-mode auto \
    --trust-remote-code False \
    --allowed-local-media-path <unset> \
    --download-dir <unset> \
    --load-format auto \
    --config-format auto \
    --dtype auto \
    --kv-cache-dtype auto \
    --max-model-len <unset> \
    --guided-decoding-backend <unset> \
    --logits-processor-pattern <unset> \
    --model-impl auto \
    --distributed-executor-backend <unset> \
    --pipeline-parallel-size <unset> \
    --tensor-parallel-size <unset> \
    --data-parallel-size <unset> \
    --enable-expert-parallel False \
    --max-parallel-loading-workers <unset> \
    --ray-workers-use-nsight False \
    --block-size <unset> \
    --enable-prefix-caching False \
    --prefix-caching-hash-algo builtin \
    --disable-sliding-window False \
    --use-v2-block-manager False \
    --num-lookahead-slots <unset> \
    --seed <unset> \
    --swap-space <unset> \
    --cpu-offload-gb <unset> \
    --gpu-memory-utilization 0.9 \
    --num-gpu-blocks-override <unset> \
    --max-num-batched-tokens <unset> \
    --max-num-partial-prefills 1 \
    --max-long-partial-prefills 1 \
    --long-prefill-token-threshold 0 \
    --max-num-seqs <unset> \
    --max-logprobs 20 \
    --disable-log-stats False \
    --quantization None \
    --rope-scaling <unset> \
    --rope-theta <unset> \
    --hf-overrides <unset> \
    --enforce-eager False \
    --max-seq-len-to-capture 8192 \
    --disable-custom-all-reduce False \
    --tokenizer-pool-size 0 \
    --tokenizer-pool-type ray \
    --tokenizer-pool-extra-config <unset> \
    --limit-mm-per-prompt <unset> \
    --mm-processor-kwargs <unset> \
    --disable-mm-preprocessor-cache False \
    --enable-lora False \
    --enable-lora-bias False \
    --max-loras 1 \
    --max-lora-rank 16 \
    --lora-extra-vocab-size 256 \
    --lora-dtype auto \
    --long-lora-scaling-factors <unset> \
    --max-cpu-loras <unset> \
    --fully-sharded-loras False \
    --enable-prompt-adapter False \
    --max-prompt-adapters 1 \
    --max-prompt-adapter-token 0 \
    --device auto \
    --num-scheduler-steps 1 \
    --use-tqdm-on-load True \
    --multi-step-stream-outputs True \
    --scheduler-delay-factor 0.0 \
    --enable-chunked-prefill False \
    --speculative-config <unset> \
    --speculative-model <unset> \
    --speculative-model-quantization None \
    --num-speculative-tokens <unset> \
    --speculative-disable-mqa-scorer False \
    --speculative-draft-tensor-parallel-size <unset> \
    --speculative-max-model-len <unset> \
    --speculative-disable-by-batch-size <unset> \
    --ngram-prompt-lookup-max <unset> \
    --ngram-prompt-lookup-min <unset> \
    --spec-decoding-acceptance-method rejection_sampler \
    --typical-acceptance-sampler-posterior-threshold <unset> \
    --typical-acceptance-sampler-posterior-alpha <unset> \
    --disable-logprobs-during-spec-decoding False \
    --model-loader-extra-config <unset> \
    --ignore-patterns [] \
    --preemption-mode <unset> \
    --served-model-name <unset> \
    --qlora-adapter-name-or-path <unset> \
    --show-hidden-metrics-for-version <unset> \
    --otlp-traces-endpoint <unset> \
    --collect-detailed-traces <unset> \
    --disable-async-output-proc False \
    --scheduling-policy fcfs \
    --scheduler-cls vllm.core.scheduler.Scheduler \
    --override-neuron-config <unset> \
    --override-pooler-config <unset> \
    --compilation-config 0 \
    --kv-transfer-config <unset> \
    --worker-cls auto \
    --worker-extension-cls "" \
    --generation-config auto \
    --override-generation-config <unset> \
    --enable-sleep-mode False \
    --calculate-kv-scales False \
    --additional-config <unset> \
    --enable-reasoning False \
    --reasoning-parser <unset> \
    --disable-cascade-attn False
